{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define weight,bias\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder\n",
    "xs = tf.placeholder(tf.float32,[None,9000])\n",
    "ys = tf.placeholder(tf.float32,[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "xs_image = tf.reshape(xs,[-1,30,300,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cnn layer\n",
    "def cov2d(W,x):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define max-pooling\n",
    "def  max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1 layer\n",
    "#每一个层都有自己单独的weight和bias \n",
    "w_conv1 = weight_variable([5,5,1,32])\n",
    "bias_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(cov2d(w_conv1,xs_image)+bias_conv1)\n",
    "p_conv1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv2\n",
    "w_conv2 = weight_variable([5,5,32,64])\n",
    "bias_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(cov2d(w_conv2,p_conv1)+bias_conv2)\n",
    "p_conv2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func1\n",
    "w_fc1 = weight_variable([7*7*64,1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "#flat\n",
    "h_pool_flat = tf.reshape(p_conv2,[-1,7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func2\n",
    "w_fc2 = weight_variable([1024,1])\n",
    "b_fc2 = bias_variable([1])\n",
    "\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,w_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define accuracy\n",
    "\n",
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global prediction\n",
    "    y_pred = sess.run(prediction,feed_dict={xs:v_xs,keep_prob:1})\n",
    "    correct = tf.equal(tf.argmax(y_pred,1),tf.argmax(v_ys,1))\n",
    "    correct_accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    result = sess.run(correct_accuracy,feed_dict={xs:v_xs,ys:v_ys,keep_prob:1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-388cf9312d0e>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cost\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=ys)\n",
    "\n",
    "\n",
    "#train\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/bert_test.txt',delimiter='\\t',names=[\"label\",\"review\"])\n",
    "val = pd.read_csv('./data/bert_val.txt',delimiter='\\t',names=[\"label\",\"review\"])\n",
    "train = pd.read_csv('./data/bert_train.txt',delimiter='\\t',names=[\"label\",\"review\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 2), (9000, 2), (18000, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape,train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "x_test = test.review\n",
    "y_test = test.label\n",
    "x_train = train.review\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary\n",
    "def loadWord2Vec(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    cnt = 0\n",
    "    fr = open(filename,'r')\n",
    "    line = fr.readline().strip()\n",
    "    #print line\n",
    "    word_dim = int(line.split(' ')[1])    \n",
    "    vocab.append(\"unk\")\n",
    "    embd.append([0]*word_dim)\n",
    "    for line in fr :\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print(\"loaded word2vec\") \n",
    "    fr.close()\n",
    "    return vocab,embd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab,embd = loadWord2Vec('/Users/james/Downloads/sgns.zhihu.word')\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividesentence(a):\n",
    "    sentence = []\n",
    "    seg_list = jieba.cut(a, cut_all=True)\n",
    "    for item in seg_list:\n",
    "        sentence.append(item)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findemdding(str1):\n",
    "    for i in range(0,vocab_size):\n",
    "        if vocab[i] == str1:\n",
    "            return embedding[i]\n",
    "    else:\n",
    "        return np.zeros(shape=(300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinevector(b):\n",
    "    sentenceembed = []\n",
    "    if len(b)<=30:\n",
    "        for i in range(0,len(b)):\n",
    "            c = findemdding(b[i])\n",
    "            sentenceembed.append(c)\n",
    "        return i,np.array(sentenceembed)\n",
    "    else:\n",
    "        for i in range(0,30):\n",
    "            c = findemdding(b[i])\n",
    "            sentenceembed.append(c)\n",
    "        return i,np.array(sentenceembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvect(a):\n",
    "    b = dividesentence(a)\n",
    "    count, arr = combinevector(b)\n",
    "    if count<=30:\n",
    "        a =  np.zeros(shape=(30-count-1,300))\n",
    "        d = np.insert(arr ,count+1,values=a,axis=0)\n",
    "        return d\n",
    "    else:\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vec = []\n",
    "for i in range(0,3000):\n",
    "    a = getvect(x_test[i])\n",
    "    print(i)\n",
    "    b = a.reshape([1,9000])\n",
    "    print(b)\n",
    "    x_test_vec.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = []\n",
    "for i in range(0,8000):\n",
    "    a = getvect(x_train[i])\n",
    "    print(i)\n",
    "    b = a.reshape([1,9000])\n",
    "    print(b)\n",
    "    x_train_vec.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec_array =  np.array(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec_array_reshape=x_train_vec_array.reshape([8000,9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec = []\n",
    "for i in range(0,8000):\n",
    "    a=np.array(y_train[i]).reshape([1,1])\n",
    "    y_train_vec.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec_array = np.array(y_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec_array_reshape = y_train_vec_array.reshape([8000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        \n",
    "        sess.run(train_op,feed_dict={xs:x_train_vec_array_reshape,ys:y_train_vec_array_reshape,keep_prob:0.5})\n",
    "        #if i % 50 == 0:\n",
    "            #print(compute_accuracy(np.array(x_test_vec),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
