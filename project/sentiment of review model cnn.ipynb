{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define weight,bias\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder\n",
    "xs = tf.placeholder(tf.float32,[None,30,300,1])\n",
    "ys = tf.placeholder(tf.float32,[None,3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cnn layer\n",
    "def cov2d(W,x):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define max-pooling\n",
    "def  max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1 layer\n",
    "#每一个层都有自己单独的weight和bias \n",
    "w_conv1 = weight_variable([5,5,1,32])\n",
    "bias_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(cov2d(w_conv1,xs)+bias_conv1)\n",
    "p_conv1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv2\n",
    "w_conv2 = weight_variable([5,5,32,64])\n",
    "bias_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(cov2d(w_conv2,p_conv1)+bias_conv2)\n",
    "p_conv2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func1\n",
    "w_fc1 = weight_variable([5*5*64,1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "#flat\n",
    "h_pool_flat = tf.reshape(p_conv2,[-1,5*5*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func2\n",
    "w_fc2 = weight_variable([1024,3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,w_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define accuracy\n",
    "\n",
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global prediction\n",
    "    y_pred = sess.run(prediction,feed_dict={xs:v_xs,keep_prob:1})\n",
    "    correct = tf.equal(tf.argmax(y_pred,1),tf.argmax(v_ys,1))\n",
    "    correct_accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    result = sess.run(correct_accuracy,feed_dict={xs:v_xs,ys:v_ys,keep_prob:1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=ys)\n",
    "\n",
    "\n",
    "#train\n",
    "#with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/bert_test.txt',delimiter='\\t',names=[\"label\",\"review\"])\n",
    "val = pd.read_csv('./data/bert_val.txt',delimiter='\\t',names=[\"label\",\"review\"])\n",
    "train = pd.read_csv('./data/bert_train.txt',delimiter='\\t',names=[\"label\",\"review\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 2), (9000, 2), (18000, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape,train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "x_test = test.review\n",
    "y_test = test.label\n",
    "x_train = train.review\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary\n",
    "def loadWord2Vec(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    cnt = 0\n",
    "    fr = open(filename,'r')\n",
    "    line = fr.readline().strip()\n",
    "    #print line\n",
    "    word_dim = int(line.split(' ')[1])    \n",
    "    vocab.append(\"unk\")\n",
    "    embd.append([0]*word_dim)\n",
    "    for line in fr :\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print(\"loaded word2vec\") \n",
    "    fr.close()\n",
    "    return vocab,embd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded word2vec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab,embd = loadWord2Vec('/Users/james/Downloads/sgns.zhihu.word')\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividesentence(a):\n",
    "    sentence = []\n",
    "    seg_list = jieba.cut(a, cut_all=True)\n",
    "    for item in seg_list:\n",
    "        sentence.append(item)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findemdding(str1):\n",
    "    for i in range(0,vocab_size):\n",
    "        if vocab[i] == str1:\n",
    "            return embedding[i]\n",
    "    else:\n",
    "        return np.zeros(shape=(300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinevector(b):\n",
    "    sentenceembed = []\n",
    "    if len(b)<=30:\n",
    "        for i in range(0,len(b)):\n",
    "            c = findemdding(b[i])\n",
    "            sentenceembed.append(c)\n",
    "        return i,np.array(sentenceembed)\n",
    "    else:\n",
    "        for i in range(0,30):\n",
    "            c = findemdding(b[i])\n",
    "            sentenceembed.append(c)\n",
    "        return i,np.array(sentenceembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvect(a):\n",
    "    b = dividesentence(a)\n",
    "    count, arr = combinevector(b)\n",
    "    if count<=30:\n",
    "        a =  np.zeros(shape=(30-count-1,300))\n",
    "        d = np.insert(arr ,count+1,values=a,axis=0)\n",
    "        return d\n",
    "    else:\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y_train_one_hot = encoder.fit_transform(y_train)\n",
    "y_test_one_hot = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train_one_hot).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/vr/3b_f9c5j0yvgq7_41pp6_ktm0000gn/T/jieba.cache\n",
      "Loading model cost 0.884 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x_train1=getvect(x_train[1])\n",
    "x_train1= np.array(x_train1).reshape([1,30,300,1])\n",
    "y_train1 = y_train_one_hot[1]\n",
    "y_train1=np.array(y_train1).reshape([1,3])\n",
    "x_test1=getvect(x_test[1])\n",
    "x_test1= np.array(x_test1).reshape([1,30,300,1])\n",
    "y_test1 = y_test_one_hot[1]\n",
    "y_test1=np.array(y_test1).reshape([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 30, 300, 1), (1, 3), (1, 30, 300, 1), (1, 3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.shape,y_train1.shape,x_test1.shape,y_test1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vec = []\n",
    "for i in range(0,3000):\n",
    "    a = getvect(x_test[i])\n",
    "    a = np.array(a).reshape([30,300,1])\n",
    "    x_test_vec.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test_array=np.array(x_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_vec = []\n",
    "for i in range(0,3000):\n",
    "    a = y_test_one_hot[i]\n",
    "    y_test_vec.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_array=np.array(y_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data  first 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    a = getvect(x_train[i])\n",
    "    b = np.array(a).reshape([30,300,1])\n",
    "    x_train_vec.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array=np.array(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec = []\n",
    "for i in range(0,2):\n",
    "    a = y_train_one_hot[i]\n",
    "    y_train_vec.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array=np.array(y_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = []\n",
    "for i in range(0,8000):\n",
    "    a = getvect(x_train[i])\n",
    "    b = np.array(a).reshape([1,30,300,1])\n",
    "    x_train_vec.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec = []\n",
    "for i in range(0,8000):\n",
    "    a = y_train_one_hot[i]\n",
    "    y_train_vec.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_array=np.array(x_train_vec)\n",
    "#y_train_array=np.array(y_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 30, 300, 1), (3000, 3))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_array.shape,y_test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 30, 300, 1), (8000, 3))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_array.shape, y_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Batch(data, label, batch_size):\n",
    "    print(data.shape, label.shape)\n",
    "    input_queue = tf.train.slice_input_producer([data, label], num_epochs=1, shuffle=True, capacity=32 ) \n",
    "    x_batch, y_batch = tf.train.batch(input_queue, batch_size=batch_size, num_threads=1, capacity=32, allow_smaller_final_batch=False)\n",
    "    return x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_Batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8df222bd002d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_Batch' is not defined"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_Batch(x_train_array,y_train_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_xs,batch_ys = tf.train.batch([x_train_array,y_train_array],batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_xs.shape,batch_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 300, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_vec[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train_vec[1]).reshape([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-61-e113a19e5a58>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-e113a19e5a58>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    #print(compute_accuracy(x_test_array,y_test_array))\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#sess.run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        \n",
    "        sess.run(train_op,feed_dict={xs:x_train_array,ys:y_train_array,keep_prob:0.5})\n",
    "        if i % 50 == 0:\n",
    "            #print(compute_accuracy(x_test_array,y_test_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
