{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:00<00:00, 23945.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/politics.yml\n",
      "dataset/history.yml\n",
      "dataset/psychology.yml\n",
      "dataset/literature.yml\n",
      "dataset/money.yml\n",
      "dataset/trivia.yml\n",
      "dataset/gossip.yml\n",
      "dataset/humor.yml\n",
      "dataset/greetings.yml\n",
      "dataset/sports.yml\n",
      "dataset/movies.yml\n",
      "dataset/science.yml\n",
      "The trimed vocabulary is: 229\n",
      "The total size of our vocabulary is: 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 231/231 [00:00<00:00, 40632.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have pretrained-vectors in vocab is: 211\n",
      "do not have pretrained-vectors in vocab is : 20\n",
      "(231, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is : (730, 300)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distancelist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3db8097ad5fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistancelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKMeansCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3db8097ad5fc>\u001b[0m in \u001b[0;36mKMeansCluster\u001b[0;34m(vectors, noofclusters, noofiterations)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0massignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massignments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistancelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m############利用seaborn画图###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distancelist' is not defined"
     ]
    }
   ],
   "source": [
    "from preparedata import compoundtxt\n",
    "from vocabulary import *\n",
    "from data import *\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.linalg import cholesky\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import choice, shuffle\n",
    "from numpy import array\n",
    "%matplotlib inline\n",
    "from model_tensorflow import KMeansCluster\n",
    "from model_tensorflow import graph\n",
    "def getdata():\n",
    "    data, vocab=get_vocabulary(\"dataset/\",1)\n",
    "    word_to_token=get_index(vocab)\n",
    "    sentence_token=get_sentence_token(data,20,word_to_token)\n",
    "    word_to_vec=loadword2vec(vocab)\n",
    "    VOCAB_SIZE = len(vocab)  # 231\n",
    "    EMBEDDING_SIZE = 300\n",
    "    static_embeddings=word_matrix(231,300,word_to_token,word_to_vec)\n",
    "    print(static_embeddings.shape)\n",
    "    input=get_data_tokens(static_embeddings,sentence_token)\n",
    "    print(\"input size is : {}\".format(input.shape))\n",
    "   \n",
    "    return input\n",
    "def KMeansCluster(vectors, noofclusters,noofiterations):\n",
    "    \"\"\"\n",
    "    K-Means Clustering using TensorFlow.\n",
    "    `vertors`应该是一个n*k的二维的NumPy的数组，其中n代表着K维向量的数目\n",
    "    'noofclusters' 代表了待分的集群的数目，是一个整型值\n",
    "    \"\"\"\n",
    "   \n",
    "    ###获得圆心\n",
    "    \n",
    "    noofclusters = int(noofclusters)\n",
    "    #分为k类没有任何意义\n",
    "    assert noofclusters < len(vectors)\n",
    "    #找出每个向量的维度\n",
    "    dim = len(vectors[0]) #300\n",
    "    #辅助随机地从可得的向量中选取中心点\n",
    "    vector_indices = list(range(len(vectors))) # 形成0-k值的list\n",
    "    shuffle(vector_indices)  #洗牌\n",
    "   \n",
    "    \n",
    "     ###计算图\n",
    "        \n",
    "    #我们创建了一个默认的计算流的图用于整个算法中，\n",
    "    #这样就保证了当函数被多次调用时，\n",
    "    #默认的图并不会被从上一次调用时留下的未使用的OPS或者Variables挤满\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        #开启计算图对话\n",
    "        sess = tf.Session()\n",
    "        ##构建基本的计算元素\n",
    "        ##首先我们需要保证每个中心点都会存在一个Variable矩阵\n",
    "        ##从现有的点集合中抽取出一部分作为默认的中心点\n",
    "        centroids = [tf.Variable((vectors[vector_indices[i]]))\n",
    "        for i in range(noofclusters)]\n",
    "        \n",
    "        ##创建一个placeholder用于存放各个中心点可能的分类的情况\n",
    "        centroid_value = tf.placeholder(\"float64\", [dim])\n",
    "        cent_assigns = []\n",
    "        ##构建tf格式的圆心\n",
    "        for centroid in centroids:\n",
    "            cent_assigns.append(tf.assign(centroid, centroid_value))\n",
    "        ##对于每个独立向量的分属的类别设置为默认值0\n",
    "        assignments = [tf.Variable(0) for i in range(len(vectors))]\n",
    "        ##这些节点在后续的操作中会被分配到合适的值\n",
    "        assignment_value = tf.placeholder(\"int32\")\n",
    "        cluster_assigns = []\n",
    "        for assignment in assignments:\n",
    "            cluster_assigns.append(tf.assign(assignment,\n",
    "                                             assignment_value))\n",
    "        \n",
    "        ##下面创建用于计算平均值的操作节点\n",
    "        #输入的placeholder\n",
    "        mean_input = tf.placeholder(\"float\", [None, dim])\n",
    "        #节点/OP接受输入，并且计算0维度的平均值，譬如输入的向量列表\n",
    "        #tf.reduce_mean 函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，\n",
    "        mean_op = tf.reduce_mean(mean_input, 0)\n",
    "        ##用于计算欧几里得距离的节点\n",
    "        v1 = tf.placeholder(\"float\", [dim])\n",
    "        v2 = tf.placeholder(\"float\", [dim])\n",
    "        euclid_dist = tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(\n",
    "            v1, v2), 2)))\n",
    "        ##这个OP会决定应该将向量归属到哪个节点\n",
    "        ##基于向量到中心点的欧几里得距离\n",
    "        #Placeholder for input\n",
    "        centroid_distances = tf.placeholder(\"float\", [noofclusters])\n",
    "        cluster_assignment = tf.argmin(centroid_distances, 0)\n",
    "        \n",
    "        ##初始化所有的状态值\n",
    "         ##这会帮助初始化图中定义的所有Variables。Variable-initializer应该定\n",
    "         ##义在所有的Variables被构造之后，这样所有的Variables才会被纳入初始化\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        #初始化所有的变量\n",
    "        sess.run(init_op)\n",
    "        ##集群遍历\n",
    "        #接下来在K-Means聚类迭代中使用最大期望算法。为了简单起见，只让它执行固\n",
    "        #定的次数，而不设置一个终止条件\n",
    "        #distancelist=[]\n",
    "        for iteration_n in range(noofiterations):\n",
    "             ##期望步骤\n",
    "            ##基于上次迭代后算出的中心点的未知\n",
    "            ##the _expected_ centroid assignments.\n",
    "            #首先遍历所有的向量\n",
    "            for vector_n in range(len(vectors)):\n",
    "                vect = vectors[vector_n]\n",
    "                #计算给定向量与分配的中心节点之间的欧几里得距离\n",
    "                distances = [sess.run(euclid_dist, feed_dict={\n",
    "                    v1: vect, v2: sess.run(centroid)})\n",
    "                             for centroid in centroids]\n",
    "                #distancelist.append(distances)\n",
    "                #下面可以使用集群分配操作，将上述的距离当做输入\n",
    "                assignment = sess.run(cluster_assignment, feed_dict = {\n",
    "                    centroid_distances: distances})\n",
    "                 #接下来为每个向量分配合适的值\n",
    "                sess.run(cluster_assigns[vector_n], feed_dict={\n",
    "                    assignment_value: assignment})\n",
    "                \n",
    "            ##最大化的步骤\n",
    "            #基于上述的期望步骤，计算每个新的中心点的距离从而使集群内的平方和最小\n",
    "            for cluster_n in range(noofclusters):\n",
    "                #收集所有分配给该集群的向量\n",
    "                assigned_vects = [vectors[i] for i in range(len(vectors))\n",
    "                                  if sess.run(assignments[i]) == cluster_n]\n",
    "                #计算新的集群中心点\n",
    "                new_location = sess.run(mean_op, feed_dict={\n",
    "                    mean_input: array(assigned_vects)})\n",
    "                #为每个向量分配合适的中心点\n",
    "                sess.run(cent_assigns[cluster_n], feed_dict={\n",
    "                    centroid_value: new_location})\n",
    "                  #返回中心节点和分组\n",
    "            centroids = sess.run(centroids)\n",
    "            assignments = sess.run(assignments)\n",
    "            return centroids, assignments\n",
    "\n",
    "            ############利用seaborn画图###############\n",
    "def graph(result,input):\n",
    "    res={\"x\":[],\"y\":[],\"kmeans_res\":[]}\n",
    "    for i in range(len(result)):\n",
    "        res[\"x\"].append(input[i][0])\n",
    "        res[\"y\"].append(input[i][1])\n",
    "        res[\"kmeans_res\"].append(result[i])\n",
    "    pd_res=pd.DataFrame(res)\n",
    "    sns.lmplot(\"x\",\"y\",data=pd_res,fit_reg=False,size=5,hue=\"kmeans_res\")\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    input=getdata()\n",
    "    center,result=KMeansCluster(input,10,50)\n",
    "    \n",
    "    graph(result,input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
