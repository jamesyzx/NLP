{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/james/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-63b32f960fa4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/james/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/james/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/james/machinelearning/assignment/writedata/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/james/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/james/machinelearning/assignment/writedata/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/james/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /Users/james/machinelearning/assignment/writedata/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/james/machinelearning/assignment/writedata/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/james/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/james/machinelearning/assignment/writedata\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnectedLayer(object):\n",
    "    def __init__(self, input_size, output_size, \n",
    "                 activator):\n",
    "        '''\n",
    "        构造函数\n",
    "        input_size: 本层输入向量的维度\n",
    "        output_size: 本层输出向量的维度\n",
    "        activator: 激活函数\n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activator = activator\n",
    "        # 权重数组W\n",
    "        self.W = np.random.uniform(-0.1, 0.1,\n",
    "            (output_size, input_size))\n",
    "        # 偏置项b\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "        # 输出向量\n",
    "        self.output = np.zeros((output_size, 1))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_array):\n",
    "        '''\n",
    "        前向计算\n",
    "        input_array: 输入向量，维度必须等于input_size\n",
    "        '''\n",
    "        # 式2\n",
    "        self.input = input_array\n",
    "        self.output = self.activator.forward(\n",
    "            np.dot(self.W, input_array) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def backward(self, delta_array):\n",
    "        '''\n",
    "        反向计算W和b的梯度\n",
    "        delta_array: 从上一层传递过来的误差项\n",
    "        '''\n",
    "        # 式8\n",
    "        self.delta = self.activator.backward(self.input) * np.dot(\n",
    "            self.W.T, delta_array)\n",
    "        self.W_grad = np.dot(delta_array, self.input.T)\n",
    "        self.b_grad = delta_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update(self, learning_rate):\n",
    "        '''\n",
    "        使用梯度下降算法更新权重\n",
    "        '''\n",
    "        self.W += learning_rate * self.W_grad\n",
    "        self.b += learning_rate * self.b_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def dump(self):\n",
    "        print('W: %s\\nb:%s' % (self.W, self.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络类\n",
    "class Network(object):\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        构造函数\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(\n",
    "                FullConnectedLayer(\n",
    "                    layers[i], layers[i+1],\n",
    "                    SigmoidActivator()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def predict(self, sample):\n",
    "        '''\n",
    "        使用神经网络实现预测\n",
    "        sample: 输入样本\n",
    "        '''\n",
    "        output = sample\n",
    "        for layer in self.layers:\n",
    "            layer.forward(output)\n",
    "            output = layer.output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def train(self, labels, data_set, rate, epoch):\n",
    "        '''\n",
    "        训练函数\n",
    "        labels: 样本标签\n",
    "        data_set: 输入样本\n",
    "        rate: 学习速率\n",
    "        epoch: 训练轮数\n",
    "        '''\n",
    "        for i in range(epoch):\n",
    "            for d in range(len(data_set)):\n",
    "                self.train_one_sample(labels[d], \n",
    "                    data_set[d], rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def train_one_sample(self, label, sample, rate):\n",
    "        self.predict(sample)\n",
    "        self.calc_gradient(label)\n",
    "        self.update_weight(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def calc_gradient(self, label):\n",
    "        delta = self.layers[-1].activator.backward(\n",
    "            self.layers[-1].output\n",
    "        ) * (label - self.layers[-1].output)\n",
    "        for layer in self.layers[::-1]:\n",
    "            layer.backward(delta)\n",
    "            delta = layer.delta\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def update_weight(self, rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def update_weight(self, rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " def loss(self, output, label):\n",
    "        return 0.5 * ((label - output) * (label - output)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_check(self, sample_feature, sample_label):\n",
    "        '''\n",
    "        梯度检查\n",
    "        network: 神经网络对象\n",
    "        sample_feature: 样本的特征\n",
    "        sample_label: 样本的标签\n",
    "        '''\n",
    "\n",
    "        # 获取网络在当前样本下每个连接的梯度\n",
    "        self.predict(sample_feature)\n",
    "        self.calc_gradient(sample_label)\n",
    "\n",
    "        # 检查梯度\n",
    "        epsilon = 10e-4\n",
    "        for fc in self.layers:\n",
    "            for i in range(fc.W.shape[0]):\n",
    "                for j in range(fc.W.shape[1]):\n",
    "                    fc.W[i,j] += epsilon\n",
    "                    output = self.predict(sample_feature)\n",
    "                    err1 = self.loss(sample_label, output)\n",
    "                    fc.W[i,j] -= 2*epsilon\n",
    "                    output = self.predict(sample_feature)\n",
    "                    err2 = self.loss(sample_label, output)\n",
    "                    expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "                    fc.W[i,j] += epsilon\n",
    "                    print ('weights(%d,%d): expected - actural %.4e - %.4e' % (\n",
    "                        i, j, expect_grad, fc.W_grad[i,j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(args):\n",
    "    return map(\n",
    "        lambda arg: map(\n",
    "            lambda line: np.array(line).reshape(len(line), 1)\n",
    "            , arg)\n",
    "        , args\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    def __init__(self):\n",
    "        self.mask = [\n",
    "            0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80\n",
    "        ]\n",
    "\n",
    "    def norm(self, number):\n",
    "        data = map(lambda m: 0.9 if number & m else 0.1, self.mask)\n",
    "        return np.array(data).reshape(8, 1)\n",
    "\n",
    "    def denorm(self, vec):\n",
    "        binary = map(lambda i: 1 if i > 0.5 else 0, vec[:,0])\n",
    "        for i in range(len(self.mask)):\n",
    "            binary[i] = binary[i] * self.mask[i]\n",
    "        return reduce(lambda x,y: x + y, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_set():\n",
    "    normalizer = Normalizer()\n",
    "    data_set = []\n",
    "    labels = []\n",
    "    for i in range(0, 256):\n",
    "        n = normalizer.norm(i)\n",
    "        data_set.append(n)\n",
    "        labels.append(n)\n",
    "    return labels, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ratio(network):\n",
    "    normalizer = Normalizer()\n",
    "    correct = 0.0;\n",
    "    for i in range(256):\n",
    "        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n",
    "            correct += 1.0\n",
    "    print ('correct_ratio: %.2f%%' % (correct / 256 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    labels, data_set = transpose(train_data_set())\n",
    "    net = Network([8, 3, 8])\n",
    "    rate = 0.5\n",
    "    mini_batch = 20\n",
    "    epoch = 10\n",
    "    for i in range(epoch):\n",
    "        net.train(labels, data_set, rate, mini_batch)\n",
    "        print ('after epoch %d loss: %f' % (\n",
    "            (i + 1),\n",
    "            net.loss(labels[-1], net.predict(data_set[-1]))\n",
    "        ))\n",
    "        rate /= 2\n",
    "    correct_ratio(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    labels, data_set = transpose(train_data_set())\n",
    "    net = Network([8, 3, 8])\n",
    "    net.gradient_check(data_set[0], labels[0])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
